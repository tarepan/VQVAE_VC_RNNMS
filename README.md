<div align="center">

# VQ-VAE for Acoustic Unit Discovery and Voice Conversion <!-- omit in toc -->
<!-- [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)][notebook] -->
[![Paper](http://img.shields.io/badge/paper-arxiv.2005.09409-B31B1B.svg)][paper]  

</div>

Clone of official implmentation of VQ-VAE for voice conversion & acoustic unit discovery.  

<!-- generated by [Markdown All in One](https://marketplace.visualstudio.com/items?itemName=yzhang.markdown-all-in-one) -->
- [Demo](#demo)
- [How to Use](#how-to-use)
    - [Requirements](#requirements)
    - [Install](#install)
- [Original paper](#original-paper)
- [Contact](#contact)

<div align="center">
    <img width="495" height="639" alt="VQ-VAE for Acoustic Unit Discovery" 
      src="https://raw.githubusercontent.com/bshall/ZeroSpeech/master/model.png"><br>
    <sup><strong>Fig 1:</strong> VQ-VAE model architecture.</sup>
</div>

This work is based on [*VQ-VAE-WaveNet*](https://arxiv.org/abs/1901.08810), *RNN_MS* and *original VQ-VAE*.

## Demo
- [Original repo's samples](https://bshall.github.io/ZeroSpeech/)

## How to Use
<!-- ### Quick training <- omit in toc ->
Jump to **[Notebook in Google Colaboratory][notebook]**, then Run. that's all!!  
 -->
Pretrained weights for the 2019 English and Indonesian datasets can be found [here](https://github.com/bshall/ZeroSpeech/releases/tag/v0.1).

### Requirements
- PyTorch>=1.4
- [NVIDIA/apex](https://github.com/NVIDIA/apex)

### Install
```bash
pip install -r requirements.txt
```

For evaluation install [bootphon/zerospeech2020](https://github.com/bootphon/zerospeech2020).

### Data and Preprocessing

1.  Download and extract the [ZeroSpeech2020 datasets](https://download.zerospeech.com/).

2.  Download the train/test splits [here](https://github.com/bshall/ZeroSpeech/releases/tag/v0.1) 
    and extract in the root directory of the repo.
    
3.  Preprocess audio and extract train/test log-Mel spectrograms:
    ```
    python preprocess.py in_dir=/path/to/dataset dataset=[2019/english or 2019/surprise]
    ```
    Note: `in_dir` must be the path to the `2019` folder. 
    For `dataset` choose between `2019/english` or `2019/surprise`.
    Other datasets will be added in the future.
    ```
    e.g. python preprocess.py in_dir=../datasets/2020/2019 dataset=2019/english
    ```
    
### Training
   
Train the models or download pretrained weights [here](https://github.com/bshall/ZeroSpeech/releases/tag/v0.1):
```
python train.py checkpoint_dir=path/to/checkpoint_dir dataset=[2019/english or 2019/surprise]
```
```
e.g. python train.py checkpoint_dir=checkpoints/2019english dataset=2019/english
```

<!-- ### Training Speed <!- omit in toc ->
X3.37 [iter/sec] @ NVIDIA T4 Google Colaboratory (AMP+)
 -->

### Evaluation
    
#### Voice conversion

```
python convert.py checkpoint=path/to/checkpoint in_dir=path/to/wavs out_dir=path/to/out_dir synthesis_list=path/to/synthesis_list dataset=[2019/english or 2019/surprise]
```
Note: the `synthesis list` is a `json` file:
```
[
    [
        "english/test/S002_0379088085",
        "V002",
        "V002_0379088085"
    ]
]
```
containing a list of items with a) the path (relative to `in_dir`) of the source `wav` files;
b) the target speaker (see `datasets/2019/english/speakers.json` for a list of options);
and c) the target file name.
```
e.g. python convert.py checkpoint=checkpoints/2019english/model.ckpt-500000.pt in_dir=../datasets/2020/2019 out_dir=submission/2019/english/test synthesis_list=datasets/2019/english/synthesis.json dataset=2019/english
```
Voice conversion samples can be found [here](https://bshall.github.io/ZeroSpeech/).

#### ABX Score
    
1.  Encode test data for evaluation:
    ```
    python encode.py checkpoint=path/to/checkpoint out_dir=path/to/out_dir dataset=[2019/english or 2019/surprise]
    ```
    ```
    e.g. python encode.py checkpoint=checkpoints/2019english/model.ckpt-500000.pt out_dir=submission/2019/english/test dataset=2019/english
    ```
    
2. Run ABX evaluation script (see [bootphon/zerospeech2020](https://github.com/bootphon/zerospeech2020)).

The ABX score for the pretrained english model (available [here](https://github.com/bshall/ZeroSpeech/releases/tag/v0.1)) is:
```
{
    "2019": {
        "english": {
            "scores": {
                "abx": 14.043611615570672,
                "bitrate": 412.2387509949519
            },
            "details_bitrate": {
                "test": 412.2387509949519
            },
            "details_abx": {
                "test": {
                    "cosine": 14.043611615570672,
                    "KL": 50.0,
                    "levenshtein": 35.927825062038984
                }
            }
        }
    }
}
```

## Original paper
[![Paper](http://img.shields.io/badge/paper-arxiv.2005.09409-B31B1B.svg)][paper]  
<!-- https://arxiv2bibtex.org/?q=2005.09409&format=bibtex -->
```
@misc{2005.09409,
Author = {Benjamin van Niekerk and Leanne Nortje and Herman Kamper},
Title = {Vector-quantized neural networks for acoustic unit discovery in the ZeroSpeech 2020 challenge},
Year = {2020},
Eprint = {arXiv:2005.09409},
}
```

[paper]:https://arxiv.org/abs/2005.09409
<!-- [notebook]:https://colab.research.google.com/github/tarepan/Scyclone-PyTorch/blob/main/Scyclone_PyTorch.ipynb -->

## Contact
Please check [original repository](https://github.com/bshall/ZeroSpeech).  
